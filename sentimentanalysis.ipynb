{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "! pip install tensorflow"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MUDhGGVvYTmV",
        "outputId": "0666a3b4-39f5-4650-f8d6-eea46825fd7f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.18.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (5.29.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.13.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.71.0)\n",
            "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.18.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.0.2)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.13.0)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.1.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.4.26)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.8)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.19.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install symspellpy\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rOyT26bWYe9a",
        "outputId": "84ea8df3-6110-48dd-cf34-db24b541160f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting symspellpy\n",
            "  Downloading symspellpy-6.9.0-py3-none-any.whl.metadata (3.9 kB)\n",
            "Collecting editdistpy>=0.1.3 (from symspellpy)\n",
            "  Downloading editdistpy-0.1.5-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.9 kB)\n",
            "Downloading symspellpy-6.9.0-py3-none-any.whl (2.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m25.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading editdistpy-0.1.5-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (144 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m144.1/144.1 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: editdistpy, symspellpy\n",
            "Successfully installed editdistpy-0.1.5 symspellpy-6.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download GloVe 6B embeddings (100D is small and good for Colab)\n",
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip -q glove.6B.zip\n",
        "\n",
        "# Load 100D GloVe\n",
        "embedding_index = {}\n",
        "with open('glove.6B.100d.txt', encoding='utf8') as f:\n",
        "    for line in f:\n",
        "        values = line.split()\n",
        "        word = values[0]\n",
        "        coefs = np.asarray(values[1:], dtype='float32')\n",
        "        embedding_index[word] = coefs\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zE0pEhktbuP1",
        "outputId": "1b2e6a00-78b4-4ef6-bb22-f558a7443af3"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-06-04 05:36:46--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2025-06-04 05:36:46--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2025-06-04 05:36:47--  https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip’\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  5.02MB/s    in 2m 38s  \n",
            "\n",
            "2025-06-04 05:39:26 (5.19 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import urllib.request\n",
        "\n",
        "dictionary_path = \"frequency_dictionary_en_82_765.txt\"\n",
        "if not os.path.exists(dictionary_path):\n",
        "    urllib.request.urlretrieve(\n",
        "        \"https://raw.githubusercontent.com/mammothb/symspellpy/master/symspellpy/frequency_dictionary_en_82_765.txt\",\n",
        "        dictionary_path\n",
        "    )\n"
      ],
      "metadata": {
        "id": "2DHppLq6Zg1I"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip -q glove.6B.zip\n"
      ],
      "metadata": {
        "id": "hIZ40ddRdI8E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/output.csv')\n",
        "\n",
        "print(\"Columns:\", df.columns)\n",
        "print(\"Sample data:\\n\", df.head())\n",
        "print(\"Shape:\", df.shape)\n",
        "\n",
        "if 'text' not in df.columns or 'label' not in df.columns:\n",
        "    raise ValueError(\"CSV must have 'text' and 'label' columns\")\n",
        "\n",
        "df = df[['text', 'label']]\n",
        "df.dropna(inplace=True)\n",
        "print(\"After dropping NaNs shape:\", df.shape)\n",
        "print(\"Labels before mapping:\", df['label'].unique())\n",
        "\n",
        "label_map = {'negative': 0, 'neutral': 1, 'positive': 2}\n",
        "df['label'] = df['label'].map(label_map)\n",
        "print(\"Labels after mapping:\", df['label'].unique())\n",
        "\n",
        "df = df.dropna(subset=['label'])\n",
        "print(\"After dropping NaN labels shape:\", df.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g8pDxOApgG3m",
        "outputId": "c0ea4c7b-38b7-4bcc-d8b0-b3e809a21811"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Columns: Index(['Unnamed: 0', 'text', 'sentiment', 'label'], dtype='object')\n",
            "Sample data:\n",
            "    Unnamed: 0                                               text sentiment  \\\n",
            "0           0  module , ek program hoti hai , jismen ya to so...   neutral   \n",
            "1           1  aur hamne aume samood ke pas unke bhaee saleh ...  positive   \n",
            "2           2  aur jab unhen yad dilaya jata hai , to ve yad ...   neutral   \n",
            "3           3            tumhen २०११ ka ted prize mil gaya hai\\n  positive   \n",
            "4           4  unhonne bad science karne ke lie ye delhi univ...   neutral   \n",
            "\n",
            "   label  \n",
            "0      1  \n",
            "1      2  \n",
            "2      1  \n",
            "3      2  \n",
            "4      1  \n",
            "Shape: (2766, 4)\n",
            "After dropping NaNs shape: (2766, 2)\n",
            "Labels before mapping: [1 2 0]\n",
            "Labels after mapping: [nan]\n",
            "After dropping NaN labels shape: (0, 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import string\n",
        "import os\n",
        "import urllib.request\n",
        "import pickle\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Embedding, Bidirectional, LSTM, Dense, Dropout, Layer\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "from symspellpy import SymSpell, Verbosity\n",
        "\n",
        "# Download SymSpell dictionary if not already downloaded\n",
        "dictionary_path = \"frequency_dictionary_en_82_765.txt\"\n",
        "if not os.path.exists(dictionary_path):\n",
        "    urllib.request.urlretrieve(\n",
        "        \"https://raw.githubusercontent.com/mammothb/symspellpy/master/symspellpy/frequency_dictionary_en_82_765.txt\",\n",
        "        dictionary_path\n",
        "    )\n",
        "\n",
        "# Initialize SymSpell\n",
        "sym_spell = SymSpell(max_dictionary_edit_distance=2, prefix_length=7)\n",
        "sym_spell.load_dictionary(dictionary_path, term_index=0, count_index=1)\n",
        "\n",
        "# Spell correction using SymSpell\n",
        "def correct_spelling(text):\n",
        "    corrected_words = []\n",
        "    for word in text.split():\n",
        "        suggestions = sym_spell.lookup(word, Verbosity.CLOSEST, max_edit_distance=2)\n",
        "        corrected_words.append(suggestions[0].term if suggestions else word)\n",
        "    return ' '.join(corrected_words)\n",
        "\n",
        "# Text cleaning\n",
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'http\\S+|www.\\S+', '', text)\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    text = correct_spelling(text)\n",
        "    return text\n",
        "\n",
        "# Load dataset\n",
        "df = pd.read_csv('/content/output.csv')\n",
        "\n",
        "# Select relevant columns\n",
        "df = df[['text', 'label']]\n",
        "df.dropna(inplace=True)\n",
        "\n",
        "print(\"Shape after dropping NaNs:\", df.shape)\n",
        "print(\"Unique labels:\", df['label'].unique())\n",
        "\n",
        "# Labels are already numeric, convert to int\n",
        "df['label'] = df['label'].astype(int)\n",
        "\n",
        "# Clean text\n",
        "df['cleaned'] = df['text'].apply(clean_text)\n",
        "\n",
        "# Tokenization\n",
        "tokenizer = Tokenizer(num_words=10000, oov_token='<OOV>')\n",
        "tokenizer.fit_on_texts(df['cleaned'])\n",
        "\n",
        "with open('tokenizer.pkl', 'wb') as f:\n",
        "    pickle.dump(tokenizer, f)\n",
        "\n",
        "sequences = tokenizer.texts_to_sequences(df['cleaned'])\n",
        "padded = pad_sequences(sequences, maxlen=100, padding='post')\n",
        "\n",
        "# Load GloVe embeddings (make sure glove.6B.100d.txt is downloaded and available)\n",
        "embedding_index = {}\n",
        "with open('glove.6B.100d.txt', encoding='utf8') as f:\n",
        "    for line in f:\n",
        "        values = line.split()\n",
        "        word = values[0]\n",
        "        coefs = np.asarray(values[1:], dtype='float32')\n",
        "        embedding_index[word] = coefs\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "embedding_dim = 100\n",
        "embedding_matrix = np.zeros((10000, embedding_dim))\n",
        "\n",
        "for word, i in word_index.items():\n",
        "    if i < 10000:\n",
        "        embedding_vector = embedding_index.get(word)\n",
        "        if embedding_vector is not None:\n",
        "            embedding_matrix[i] = embedding_vector\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    padded, df['label'], test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "print(\"y_train unique values:\", np.unique(y_train))\n",
        "print(\"y_test unique values:\", np.unique(y_test))\n",
        "\n",
        "# Custom Attention Layer to fix KerasTensor error\n",
        "class SimpleAttention(Layer):\n",
        "    def __init__(self, **kwargs):\n",
        "        super(SimpleAttention, self).__init__(**kwargs)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        # inputs shape: (batch_size, timesteps, features)\n",
        "        score = tf.nn.softmax(inputs, axis=1)  # softmax on timesteps axis\n",
        "        context_vector = tf.reduce_sum(score * inputs, axis=1)\n",
        "        return context_vector\n",
        "\n",
        "# Build model\n",
        "input_layer = Input(shape=(100,))\n",
        "embedding_layer = Embedding(\n",
        "    input_dim=10000,\n",
        "    output_dim=embedding_dim,\n",
        "    weights=[embedding_matrix],\n",
        "    trainable=False\n",
        ")(input_layer)\n",
        "lstm_layer = Bidirectional(LSTM(64, return_sequences=True))(embedding_layer)\n",
        "attention_output = SimpleAttention()(lstm_layer)\n",
        "dropout_layer = Dropout(0.5)(attention_output)\n",
        "output_layer = Dense(3, activation='softmax')(dropout_layer)\n",
        "\n",
        "model = Model(inputs=input_layer, outputs=output_layer)\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Train model\n",
        "early_stop = EarlyStopping(monitor='val_loss', patience=3)\n",
        "\n",
        "model.fit(X_train, y_train, epochs=10, validation_split=0.2, callbacks=[early_stop], batch_size=64)\n",
        "\n",
        "# Ensure models directory exists\n",
        "os.makedirs('models', exist_ok=True)\n",
        "model.save('models/lstm_model.h5')\n",
        "\n",
        "# Evaluate\n",
        "y_pred = model.predict(X_test)\n",
        "y_pred = np.argmax(y_pred, axis=1)\n",
        "print(classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DCjG3yWcedFz",
        "outputId": "bf00dcd1-a37a-495b-f28d-6050e43c254a"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape after dropping NaNs: (2766, 2)\n",
            "Unique labels: [1 2 0]\n",
            "y_train unique values: [0 1 2]\n",
            "y_test unique values: [0 1 2]\n",
            "Epoch 1/10\n",
            "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 294ms/step - accuracy: 0.4177 - loss: 1.0738 - val_accuracy: 0.5756 - val_loss: 0.9432\n",
            "Epoch 2/10\n",
            "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 181ms/step - accuracy: 0.5409 - loss: 0.9756 - val_accuracy: 0.5914 - val_loss: 0.9141\n",
            "Epoch 3/10\n",
            "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 242ms/step - accuracy: 0.5418 - loss: 0.9648 - val_accuracy: 0.6005 - val_loss: 0.9057\n",
            "Epoch 4/10\n",
            "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 189ms/step - accuracy: 0.5390 - loss: 0.9342 - val_accuracy: 0.5914 - val_loss: 0.9009\n",
            "Epoch 5/10\n",
            "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 191ms/step - accuracy: 0.5663 - loss: 0.9370 - val_accuracy: 0.6050 - val_loss: 0.8813\n",
            "Epoch 6/10\n",
            "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 278ms/step - accuracy: 0.5863 - loss: 0.8961 - val_accuracy: 0.6117 - val_loss: 0.8748\n",
            "Epoch 7/10\n",
            "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 181ms/step - accuracy: 0.5868 - loss: 0.8772 - val_accuracy: 0.6117 - val_loss: 0.8739\n",
            "Epoch 8/10\n",
            "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 262ms/step - accuracy: 0.6121 - loss: 0.8462 - val_accuracy: 0.6005 - val_loss: 0.9044\n",
            "Epoch 9/10\n",
            "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 232ms/step - accuracy: 0.6102 - loss: 0.8394 - val_accuracy: 0.5847 - val_loss: 0.9010\n",
            "Epoch 10/10\n",
            "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 184ms/step - accuracy: 0.6444 - loss: 0.7971 - val_accuracy: 0.6027 - val_loss: 0.8777\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 53ms/step\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.45      0.15      0.22       121\n",
            "           1       0.62      0.67      0.64       238\n",
            "           2       0.46      0.61      0.53       195\n",
            "\n",
            "    accuracy                           0.53       554\n",
            "   macro avg       0.51      0.48      0.46       554\n",
            "weighted avg       0.53      0.53      0.51       554\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_sentiment(text, tokenizer, model, max_length=100):\n",
        "    # Preprocess input text if needed (lowercase, remove punctuation etc.) based on your original pipeline\n",
        "    text = text.lower()\n",
        "    text = ''.join([c for c in text if c not in string.punctuation])\n",
        "\n",
        "    # Tokenize and pad the text\n",
        "    sequences = tokenizer.texts_to_sequences([text])\n",
        "    padded = pad_sequences(sequences, maxlen=max_length, padding='post', truncating='post')\n",
        "\n",
        "    # Predict sentiment\n",
        "    prediction = model.predict(padded)\n",
        "\n",
        "    # Assuming binary sentiment classification (0=negative,1=positive)\n",
        "    sentiment = \"Positive\" if prediction[0][0] > 0.5 else \"Negative\"\n",
        "\n",
        "    return sentiment\n",
        "\n",
        "# Example usage:\n",
        "while True:\n",
        "    user_input = input(\"Enter text to analyze sentiment (or 'exit' to quit): \")\n",
        "    if user_input.lower() == 'exit':\n",
        "        break\n",
        "    sentiment = predict_sentiment(user_input, tokenizer, model)\n",
        "    print(f\"Sentiment: {sentiment}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qLI-4pPqiixZ",
        "outputId": "7f47ad6f-8f19-49f5-873d-304b9d6b2ac0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter text to analyze sentiment (or 'exit' to quit): ye product acha nahi hai\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step\n",
            "Sentiment: Negative\n"
          ]
        }
      ]
    }
  ]
}